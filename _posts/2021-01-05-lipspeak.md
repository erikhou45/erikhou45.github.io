---
title: "LipSpeak"
date: 2021-01-05T15:34:30-04:00
categories:
  - UC Berkeley Academic Project
tags:
  - CNN
  - computer vision
  - mobile development
  - deep learning
---

An easy-to-use mobile application to help people with voice disorders communicate effectively

## Description

LipSpeak is focused on helping people with voice disorders, a condition affecting 7.5 million patients in the US alone. Powered by a novel visual keyword spotting neural network architecture (developed by reseachers[1] at University of Oxford), our smartphone app MVP converts user’s lip movements to synthesized speech and is targeted for people who lost their voice due to illness or surgery but are still capable of articulative lip movements.

We intend to use our MVP to contribute to peer and industry learning about how silent speech voicing technologies can be applied to improve the quality of life for people suffering from voice impairments.

## Demo

<iframe width="560" height="315" src="https://www.youtube.com/embed/ZKcpLItRvGI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Techniques
* supervised learning (classification)
* computer vision with (CNN and LSTM)
* image embedding
* multi-GPU inference
* cloud deployment

## Tools
* PyTorch
* TensorFlow
* sckit-learn
* AWS
* Flask
* Firebase
* Firestore

## Outcome
The MVP accuracy on a 10-phrase phrasebook is 75%, thus outperforming other state-of-the-art models. Similar solutions to ours suffer from lack of generalization to different speakers [2], or are restricted to very short and limited phrases [3], or have poor lip reading model performance with high error rates [4] and [5].  

## More Information
More information can be found at the following links:

[Project Website](https://groups.ischool.berkeley.edu/LIPSPEAK/). 
[Project Page](https://www.ischool.berkeley.edu/projects/2020/lipspeak). 
[Project Repository](https://github.com/avinashsc/Lipspeak). 

## References

[1 ] Liliane Momeni, Triantafyllos Afouras, Themos Stafylakis, Samuel Albanie, Andrew Zisserman. Seeing wake words: Audio-visual keyword spotting, 2020.
[2] Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri. Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition. June 2020.
[3] Flesher W, Twamley J, Nanda S. SRAVI: A feasibility study into use of visual speech recognition to aid communication. 2020
[4] Ariel Ephrat, Tavi Halperin, and Shmuel Peleg. Improved speech reconstruction from silent video. 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pages 455–462, 2017
[5] Michael Wand, Jan Koutn´ık, and Jurgen Schmidhuber. ¨ Lipreading with long short-term memory. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6115–6119. IEEE, 2016.
